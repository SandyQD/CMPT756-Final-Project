# Copyright 2024 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Install the packages.
! pip3 install --upgrade google-cloud-aiplatform

# Automatically restart kernel after installs so that your environment can access the new packages.
import IPython

app = IPython.Application.instance()
app.kernel.do_shutdown(True)

PROJECT_ID = "cmpt756-final-project-417723"  # @param {type:"string"}

# Set the project id
! gcloud config set project {PROJECT_ID}

REGION = "us-central1"  # @param {type: "string"}

# ! gcloud auth login

from google.colab import auth
auth.authenticate_user()

# The service account for deploying fine tuned model.
# The service account looks like:
# '<account_name>@<project>.iam.gserviceaccount.com'
SERVICE_ACCOUNT = "service@cmpt756-final-project-417723.iam.gserviceaccount.com"  # @param {type:"string"}

BUCKET_URI = f"gs://cmpt756finalproject-{PROJECT_ID}-unique"  # @param {type:"string"}

! gsutil mb -l {REGION} -p {PROJECT_ID} {BUCKET_URI}

import os
from datetime import datetime

from google.cloud import aiplatform

staging_bucket = os.path.join(BUCKET_URI, "autogluon_staging")
aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=staging_bucket)

# The pre-built training docker image.
TRAIN_DOCKER_URI = "us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-autogluon-train:20240124_0927_RC00"
# The pre-built serving docker image.
SERVE_DOCKER_URI = "us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-autogluon-serve:20240124_0938_RC00"
# Serving port.
PORT = 8501

def gcs_fuse_path(path: str) -> str:
    """Try to convert path to gcsfuse path if it starts with gs:// else do not modify it."""
    path = path.strip()
    if path.startswith("gs://"):
        return "/gcs/" + path[5:]
    return path


def deploy_model(model_path):
    """Deploy the model to Vertex AI Endpoint for prediction."""
    model_name = "autogluon"
    endpoint = aiplatform.Endpoint.create(display_name=f"{model_name}-endpoint")
    serving_env = {
        "model_path": model_path,
    }
    # Since the model_id is a GCS path, use artifact_uri to pass it
    # to the serving docker.
    artifact_uri = model_path
    model = aiplatform.Model.upload(
        display_name=model_name,
        serving_container_image_uri=SERVE_DOCKER_URI,
        serving_container_ports=[PORT],
        serving_container_predict_route="/predict",
        serving_container_health_route="/ping",
        serving_container_environment_variables=serving_env,
        artifact_uri=artifact_uri,
    )
    model.deploy(
        endpoint=endpoint,
        machine_type="n1-highmem-16",
        deploy_request_timeout=1800,
        service_account=SERVICE_ACCOUNT,
    )
    return model, endpoint

# Set up training docker arguments.

TIMESTAMP = datetime.now().strftime("%Y%m%d_%H%M%S")
JOB_NAME = "pytorch_autogluon" + TIMESTAMP

finetuning_workdir = os.path.join(BUCKET_URI, JOB_NAME)
train_data_path = (
    "https://raw.githubusercontent.com/mli/ag-docs/main/knot_theory/train.csv"
)
# The column id to predict.
label = "signature"

# We are using the
docker_args_list = [
    "--train_data_path",
    train_data_path,
    "--label",
    label,
    "--model_save_path",
    f"{gcs_fuse_path(finetuning_workdir)}",
]
print(docker_args_list)

# Create and run the training job.
# Click on the generated link in the output under "View backing custom job:" to see your run in the Cloud Console.
container_uri = TRAIN_DOCKER_URI
job = aiplatform.CustomContainerTrainingJob(
    display_name=JOB_NAME,
    container_uri=container_uri,
)
model = job.run(
    args=docker_args_list,
    base_output_dir=f"{finetuning_workdir}",
    replica_count=1,
    machine_type="n1-highmem-16",
)

model, endpoint = deploy_model(model_path=finetuning_workdir)

import time
instances = [
    {
        "Unnamed: 0": 70746,
        "chern_simons": 0.0905302166938781,
        "cusp_volume": 12.226321765565215,
        "hyperbolic_adjoint_torsion_degree": 0,
        "hyperbolic_torsion_degree": 10,
        "injectivity_radius": 0.5077560544013977,
        "longitudinal_translation": 10.685555458068848,
        "meridinal_translation_imag": 1.1441915035247805,
        "meridinal_translation_real": -0.5191566348075867,
        "short_geodesic_imag_part": -2.7606005668640137,
        "short_geodesic_real_part": 1.0155121088027954,
        "Symmetry_0": 0.0,
        "Symmetry_D3": 0.0,
        "Symmetry_D4": 0.0,
        "Symmetry_D6": 0.0,
        "Symmetry_D8": 0.0,
        "Symmetry_Z/2 + Z/2": 1.0,
        "volume": 11.393224716186523,
    },
]

start = time.time()

predictions = endpoint.predict(instances=instances).predictions

print("Prediction:", predictions, "Latency:", (time.time() - start))

# Delete endpoint resource.
endpoint.delete(force=True)

# Delete model resource.
model.delete()

# Delete Cloud Storage objects that were created.
delete_bucket = False
if delete_bucket or os.getenv("IS_TESTING"):
    ! gsutil -m rm -r $BUCKET_URI
